'''
This file implements the bus factor metric as described in (ICSE-SEIP 2022, Jabrayilzade et al.)
'''
from __future__ import annotations

import os
import math
import time
import shutil
import tempfile
import datetime as dt
from pathlib import Path
from typing import Dict, Set, Tuple, List
from collections import defaultdict

from git import Repo, GitCommandError
from dotenv import load_dotenv

# Since commiting, might have to play around with it.
SINCE_DAYS_DEFAULT = 365

# Code extensions and those to skip, generated by ChatGPT
CODE_EXTS = {
    ".py", ".ipynb", ".md", ".rst", ".txt", ".json", ".yaml", ".yml", ".ini", ".toml",
    ".cfg", ".sh", ".bat", ".ps1", ".js", ".ts", ".jsx", ".tsx", ".java", ".scala",
    ".kt", ".c", ".h", ".hpp", ".hh", ".cc", ".cpp", ".m", ".mm", ".go", ".rs",
    ".rb", ".php", ".pl", ".r", ".swift", ".css", ".scss", ".html", ".xml"
}
BINARY_SKIP_EXTS = {
    ".bin", ".safetensors", ".pt", ".pth", ".onnx", ".tflite", ".pb",
    ".tar", ".gz", ".xz", ".zip", ".7z", ".rar", ".pdf"
}


# Helper functions
def _is_code_like(path: str) -> bool:
    p = Path(path)
    ext = p.suffix.lower()
    if ext in BINARY_SKIP_EXTS:
        return False
    # If it has a known text/code extension, keep it
    if ext in CODE_EXTS:
        return True
    # Otherwise, heuristically keep small-ish files without extension
    # (many repos have scripts / config without extensions)
    try:
        return ext == "" and p.stat().st_size < 512_000  # 512 KB
    except Exception:
        return False

def _first_author_email(repo: Repo, file_path: str) -> str | None:
    """
    Returns the email of the commit author who initially added the file.
    Uses: git log --diff-filter=A --reverse --format=%ae -- <file>
    """
    try:
        out = repo.git.log("--diff-filter=A", "--reverse", "--format=%ae", "--", file_path)
        line = out.splitlines()[0].strip() if out else ""
        return line or None
    except GitCommandError:
        return None
    
# Degree of Authorship calculations as per paper
def _collect_doa_inputs(repo: Repo, since_days: int) -> Tuple[
    Dict[str, Dict[str, int]],  # dl[(file)][author_email] = DL
    Dict[str, int],             # total_commits_per_file[file] = total DL + AC
    Dict[str, Set[str]],        # contributors_per_file[file] = set(author_emails)
    Dict[str, str],             # creator_email[file] = email or ""
]:
    """
    Walk recent commits and build:
      - DL per (file, author)
      - total commits per file
      - unique contributors per file
      - first author (creator) per file
    """
    since_dt = dt.datetime.utcnow() - dt.timedelta(days=since_days)
    since_arg = since_dt.strftime("%Y-%m-%d")  # e.g., "2024-09-25"


    dl: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))
    total_by_file: Dict[str, int] = defaultdict(int)
    contributors: Dict[str, Set[str]] = defaultdict(set)
    creators: Dict[str, str] = {}

    commits = list(repo.iter_commits("HEAD", since=since_arg))
    if not commits:
        # fallback: use full history if window has nothing
        commits = list(repo.iter_commits("HEAD"))

    for c in commits:
        # Prefer email; fall back to name if email missing
        author = (c.author.email or c.author.name or "unknown").strip().lower()

        # Get files touched by this commit
        try:
            changed_files = list(getattr(c.stats, "files", {}).keys())
        except Exception:
            # Some commits may fail diff stats; skip them safely
            changed_files = []

        for f in changed_files:
            if not _is_code_like(f):
                continue
            dl[f][author] += 1
            total_by_file[f] += 1
            contributors[f].add(author)

    # Fill in creators per present file (independent of window)
    for f in list(total_by_file.keys()):
        creators[f] = _first_author_email(repo, f) or ""

    return dl, total_by_file, contributors, creators

def _doa(author: str, file_path: str,
         dl: Dict[str, Dict[str, int]],
         total_by_file: Dict[str, int],
         contributors: Dict[str, Set[str]],
         creators: Dict[str, str]) -> float:
    """
    Compute DOA for (author, file) as:
      3.293 + 1.098*FA + 0.164*DL - 0.321*log(1 + AC)
    where AC = total_commits(file) - DL(author,file)
    """
    DL = dl[file_path].get(author, 0)
    AC = max(0, total_by_file[file_path] - DL)
    FA = 1 if creators.get(file_path, "").lower() == author.lower() and creators[file_path] != "" else 0

    return 3.293 + 1.098 * FA + 0.164 * DL - 0.321 * math.log(1 + AC)

def _authors_by_file(dl, total_by_file, contributors, creators) -> Dict[str, Set[str]]:
    """
    Apply the author thresholds:
      DOA > 3.293 and DOA > 0.75 * max_DOA(file)
    Returns: authors_of_file[file] = {author_emails}
    """
    authors_of_file: Dict[str, Set[str]] = {}

    for f in total_by_file.keys():
        if total_by_file[f] == 0:
            authors_of_file[f] = set()
            continue

        # Compute DOAs for all contributors of this file
        doa_by_author = {
            a: _doa(a, f, dl, total_by_file, contributors, creators)
            for a in contributors.get(f, set())
        }
        if not doa_by_author:
            authors_of_file[f] = set()
            continue

        max_doa = max(doa_by_author.values())
        keep: Set[str] = set()
        for a, doa_val in doa_by_author.items():
            if doa_val > 3.293 and doa_val > 0.75 * max_doa:
                keep.add(a)

        authors_of_file[f] = keep

    return authors_of_file

def _compute_bus_factor(authors_of_file: Dict[str, Set[str]]) -> Tuple[int, List[str]]:
    """
    Greedy algorithm:
      - Repeatedly remove the author who "authors" the most files.
      - After each removal, mark files abandoned whose authors set is now empty.
      - Stop when >50% of files are abandoned; BF = #removed.
    Returns (bus_factor, removed_authors_in_order)
    """
    files = list(authors_of_file.keys())
    if not files:
        return 0, []

    # Start with files that already have no authors (abandoned)
    abandoned: Set[str] = {f for f in files if not authors_of_file[f]}
    removed: List[str] = []

    # Current active authors set
    active_authors: Set[str] = set()
    for s in authors_of_file.values():
        active_authors |= s

    # Helper to recompute abandoned after a removal
    def recompute_abandoned(current_removed: Set[str]) -> Set[str]:
        new_abandoned = set(abandoned)
        for f in files:
            if f in new_abandoned:
                continue
            if authors_of_file[f] and authors_of_file[f].issubset(current_removed):
                new_abandoned.add(f)
        return new_abandoned

    current_removed: Set[str] = set(removed)
    while True:
        # Check stopping condition
        if len(abandoned) > 0.5 * len(files):
            return len(removed), removed

        if not active_authors:
            # No authors left to remove
            return len(removed), removed

        # Pick top author by file coverage (ties broken arbitrarily but deterministically)
        coverage_counts = {
            a: sum(1 for f in files if a in authors_of_file[f])
            for a in active_authors
        }
        top_author = max(coverage_counts.items(), key=lambda kv: kv[1])[0]

        removed.append(top_author)
        current_removed.add(top_author)
        active_authors.remove(top_author)

        # Update abandoned set after removing this author
        abandoned = recompute_abandoned(current_removed)

def _normalize_score(bus_factor: int, authors_of_file: Dict[str, Set[str]]) -> float:
    """Normalize BF to [0,1] relative to the number of active authors."""
    active_authors: Set[str] = set()
    for s in authors_of_file.values():
        active_authors |= s
    denom = max(1, len(active_authors))
    return min(1.0, bus_factor / denom)

def bus_factor(url: str, since_days: int = SINCE_DAYS_DEFAULT) -> Tuple[float, int]:
    """
    Clone the repo at `url`, compute Bus Factor score normalized to [0,1],
    and return (score, latency_ms).
    """
    start = time.time()
    temp_dir = tempfile.mkdtemp()

    try:
        # Clone (works for Hugging Face model repos as they are git-backed)
        Repo.clone_from(url, temp_dir)

        repo = Repo(temp_dir)
        dl, total_by_file, contributors, creators = _collect_doa_inputs(repo, since_days)
        # If there are no files or commits, return 0 quickly
        if not total_by_file:
            return 0.0, int((time.time() - start) * 1000)

        authors_of_file = _authors_by_file(dl, total_by_file, contributors, creators)
        bf, _removed = _compute_bus_factor(authors_of_file)
        score = _normalize_score(bf, authors_of_file)

        latency_ms = int((time.time() - start) * 1000)
        return score, latency_ms

    except Exception as e:
        # Conservative fallback on failure
        print(f"[busfactor] Error: {e}")
        return 0.0, int((time.time() - start) * 1000)
    finally:
        shutil.rmtree(temp_dir, ignore_errors=True)

# Temporary main function to run locally
if __name__ == "__main__":
    print("Starting Bus Factor computation...")
    load_dotenv(dotenv_path=Path(__file__).resolve().parents[3] / ".env")
    token = os.getenv("HF_TOKEN")
    if not token:
        print("HF_TOKEN not found; cloning public repos only.")
    else:
        print("Loaded token starts with:", token[:10])

    # Example (Hugging Face model repo)
    url = f"https://hf:{token}@huggingface.co/bert-base-uncased" if token else "https://huggingface.co/bert-base-uncased"
    # f"https://hf:{token}@huggingface.co/google/gemma-3-270m/tree/main"
    # url = f"https://hf:{token}@huggingface.co/google/gemma-3-270m/tree/main" if token else "https://huggingface.co/google/gemma-3-270m/tree/main"
    s, ms = bus_factor(url)
    print(f"Bus Factor score: {s:.3f}, latency: {ms} ms")